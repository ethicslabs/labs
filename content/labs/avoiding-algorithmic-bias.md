---
title: "Avoiding Algorithmic Bias Lab"
date: "2021-07-21"
---

Avoiding Algorithmic Bias Lab

Lab Summary

**Ethics background** required: Ideally, students should be familiar
with the 4 ethical frameworks presented in the first year curriculum
(virtue ethics, deontology, utilitarianism, analogies). If they (or the
professor) are not, a brief summary is provided in the reflection.

**Subject matter** referred to in this lab: Algorithms

**Placement** in overall ethics curriculum:

            Academic year: Year 2-4 core course that talks about
algorithms or machine learning.

> Recommended previous labs: First year ethics curriculum

            Recommended follow-up labs: No direct follow-up labs

**Time** required:

            In class: 30 minutes * *(online version is also included)

**Learning objectives**:

-   Students are introduced to or reminded of the dangers of algorithmic
    > bias in predictive algorithms

-   Students are introduced to a deontological set of guidelines to
    > evaluate an algorithm for bias

-   Students apply the guidelines to actual algorithms and practice this
    > evaluation

-   Students review the four ethical frameworks and evaluate the benefit
    > of using each in regard to evaluating predictive algorithms

* *

**Lab overview**:

<u>Ethical issues to be considered</u>: Algorithmic bias, diversity,
transparency

 

<u>Exercise flow:</u>

-   Students read and talk about algorithms with issues

-   Students are introduced to the EU Ethics Guidelines for Trustworthy
    > AI

-   Students evaluate the algorithms previously presented based on these
    > guidelines

**Preparation**

Read through the entire lab. Print enough of each of the 3 scenarios to
distribute one each to groups of 2-4 students. For example, if you have
a class of 20 students, you might want to print 2 copies of each.
Prepare a PowerPoint (or similar) slide to project with the 7 EU
Guidelines on it.

**If this lab is to be presented online**

See online addendum. Students work through the assignment and then
peer-review the assignment of another student.

Instructors Guide

**Lesson plan**

<u>Introduction</u> (5 minutes)

Ask students for the definition of “algorithm.” (Something like a
step-by-step approach to use as a guideline for producing a program that
the computer can use to solve a problem)

Remind students that their college application was probably subject to
some sort of algorithm to determine whether they were admitted. **Ask
class to suggest what some of the steps might have been** in the
decision making process.

Here are some possibilities:

-   Is the GPA > X (X depends on institution)

-   Is the SAT/ACT > X

-   5 points for each area of community service

-   3 points for faith statement

**To students:** Generally, these types of algorithms are point-based.
Why would a point-based method be adopted? What are the benefits?
(probably something about objectivity)

Do you see any problems with the point-based method? (possible
responses: not everything can be boiled down to numbers, how to decide
how many points something is worth, is it easier for some students to
meet the requirements (parents can pay for tutoring, for example))

<u>Activity 1 (5 min)</u>

Facilitate the division of students into groups of 2-4 members. Each
group should be given one of the scenarios in the student handout
section. Direct the students to read the scenario as a group and discuss
what went right and what went wrong.

<u>Preparation for activity 2 (5 min)</u>

**Professor should read or summarize after the previous group activity
is finished**

Point-based algorithms for making decisions are a rudimentary form of
Artificial Intelligence. Kaplan and Haenlein recently defined AI as “a
system's ability to correctly interpret external data, to learn from
such data, and to use those learnings to achieve specific goals and
tasks through flexible adaptation” Our example of a point-based
algorithm for college admission is used to predict who will most likely
succeed at a particular college or university. Similarly, each of the
scenarios that you read involved simple predictive algorithms.

We hear frequently of the challenges with AI and even simple predictive
algorithms share the same issues. Recently, the EU came up with a set of
guidelines to help diminish the chance of ethical issues in such
algorithms. As we go through these guidelines, think back on our college
admissions example and the example that you read in your group and see
if any of them would be helpful to consider if you were in charge of
developing such an algorithm. Be prepared to discuss your observations
with your group.

***Note to professor**: The following guidelines should be made
available to students so they can follow along. An easy approach is to
project the guidelines on a screen for the class. It is probably a good
idea for students to participate in the reading (assign 1 guideline to a
student to read) so that they are more engaged.*

EU Ethics Guidelines for Trustworthy AI

**1 Human agency and oversight**

Users should be able to make informed autonomous decisions regarding AI
systems. They should be given the knowledge and tools to comprehend and
interact with AI systems to a satisfactory degree and, where possible,
be enabled to reasonably self-assess or challenge the system. Humans are
involved in the design and governance of the system including when and
where the system is used.

**2 Technical robustness and safety**

Including resilience to attack and security, fall back plan and general
safety, accuracy, reliability with a range of inputs and reproducibility
of results with same inputs. The system is well-tested during creation
and reviewed during use.

**3 Privacy and data governance**

Including respect for privacy, quality and integrity of data, and access
to data. Data must be ethically cleaned before it is used.

**4 Transparency**

The data sets on which the decision making is based must be
well-documented with regard to origin and alteration (such as in
cleaning). Decisions made by AI must be understandable and able to be
traced. Users should have access to the decision-making process and be
aware that the decision was made, at least in part, but a computer.

**5 Diversity, non-discrimination and fairness**

Including the avoidance of unfair bias, accessibility and universal
design, and stakeholder participation. Diversity should be encouraged
and designers should consider carefully whether past positive results
(success of people with certain gender, ethnicity, personality etc.)
were related or unrelated to the success. Avoid propagation of past
bias. Systems should be user-centric and accessible to the widest range
of users. People who will be affected by the system should be involved
throughout the design of the system.

**6 Societal and environmental wellbeing**

Including sustainability and environmental friendliness, social impact,
society and democracy. Supply chain for the system should be
sustainable. Its use should impact social relationships in a negative
way. It should enhance, rather than detract from, a democratic society.

**7 Accountability**

This necessitates that mechanisms be put in place to ensure
responsibility and accountability for AI systems and their outcomes,
both before and after their development, deployment and use. Potential
impacts should be minimized and when encountered, they should be
addressed and there should be a policy for making amends to the injured
party. The system should be adjusted to eliminate the possibility of
this happening in the future. When a conflict of interest arises, the
tradeoffs made should be acknowledged.

<u>Activity 2 (5 min)</u>

Students discuss with their group observations that they made regarding
the potential benefits of applying 1 or more of the guidelines to the
college admissions scenario or to the scenario read as a group.

<u>Reflection (10 min)</u>

Have each group report on one or more of the guidelines that would have
helped in the design of either the college admission scenario or the
scenario that they read. If it was the scenario read, have the group
first summarize the scenario. It is probably best to ask if other groups
have comments on that same scenario before moving on.

**Ask the students**: Who can tell me which ethical framework is being
used by applying a set of guidelines to vet a predictive algorithm?
(virtue ethics, deontological, analogies, utilitarianism?) If they need
a refresher:

-   Virtue ethics: making a decision based on whether the outcome
    > upholds a set of virtues (honesty, compassion, patience, hard-work
    > are some examples)

-   Deontological ethics: making decisions based on a set of rules
    > (**this is the expected choice**)

-   Analogies: making a decision based on a similar situation where the
    > ethicality of the similar situation was widely accepted

-   Utilitarianism: making a decision based on what is best for the
    > majority

**Ask the students** if they believe that this was an appropriate
framework to use in this case. Why or why not?

Possible answers:

-   Probably good if used only as guidelines. It’s a good starting place

-   Virtue ethics might be a good secondary approach, and some of the
    > virtues (respect for diversity, for example) are included in the
    > guidelines

-   Analogies probably would not be a good approach as it is not a
    > single decision that must be made and it would be very difficult
    > to arrive at an appropriate analogy

-   Utilitarianism is probably what caused the problem in the first
    > place – designers would figure that it would work for most people.

<u>Assessment</u> (to be included at a later date on a quiz, paper, or
exam to determine if LOs were reached)

-   Assess the deontological approach to evaluating an algorithm for
    ethicality compared to the virtue ethics, analogy, or utilitarianism
    approach. Is it better, worse, explain.

-   Provide 2 of the guidelines listed in the EU Ethics Guidelines for
    Trustworthy AI that we talked about in class

-   Summarize one of the examples of predictive algorithms that
    exhibited bias that were presented in class. Specifically state the
    bias, and explain how it could have been avoided.

**References**

All of the scenarios were excerpted from “How Algorithms Rule Our
Working Lives” by Cathy O’Neil (author of Weapons of Math Destruction)
https://www.theguardian.com/science/2016/sep/01/how-algorithms-rule-our-working-lives

Principles are directly from the text of the ETHICS GUIDELINES FOR
TRUSTWORTHY AI compiled by the High-Level Expert Group on Artificial
Intelligence set up by the European Commission. Original was available
here

[<u>https://ai.bsa.org/wp-content/uploads/2019/09/AIHLEG\_EthicsGuidelinesforTrustworthyAI-ENpdf.pdf
on
2/3/2020</u>](https://ai.bsa.org/wp-content/uploads/2019/09/AIHLEG_EthicsGuidelinesforTrustworthyAI-ENpdf.pdf%20on%202/3/2020).

AI definition from:

Kaplan, Andreas; Haenlein, Michael (1 January 2019). "Siri, Siri, in my
hand: Who's the fairest in the land? On the interpretations,
illustrations, and implications of artificial intelligence". Business
Horizons. 62 (1): 15–25.

Additional reading: **Algorithmic bias detection and mitigation: Best
practices and policies to reduce consumer harms**

[<u>https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/</u>](https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/)
Accessed 2/3/2020

 

 

 

 

**Student handouts are attached on next pages**

 

Predictive Algorithm Scenario #1

**Algorithms as objective agents**

After the financial crash \[of 2008\], it became clear that the housing
crisis and the collapse of major financial institutions had been aided
and abetted by mathematicians wielding magic formulas. If we had been
clear-headed, we would have taken a step back at this point to figure
out how we could prevent a similar catastrophe in the future. But
instead, in the wake of the crisis, new mathematical techniques were
hotter than ever, and expanding into still more domains. They churned
24/7 through petabytes of information, much of it scraped from social
media or e-commerce websites. And increasingly they focused not on the
movements of global financial markets but on human beings, on us.
Mathematicians and statisticians were studying our desires, movements,
and spending patterns. They were predicting our trustworthiness and
calculating our potential as students, workers, lovers, criminals.

This was the big data economy, and it promised spectacular gains. A
computer program could speed through thousands of résumés or loan
applications in a second or two and sort them into neat lists, with the
most promising candidates on top. This not only saved time but also was
marketed as fair and objective. After all, it didn’t involve prejudiced
humans digging through reams of paper, just machines processing cold
numbers.

Few of the algorithms and scoring systems have been vetted with
scientific rigor, and there are good reasons to suspect they wouldn’t
pass such tests. For instance, automated teacher assessments can vary
widely from year to year, putting their accuracy in question. Tim
Clifford, a New York City middle school English teacher of 26 years, got
a 6 out of 100 in one year and a 96 the next, without changing his
teaching style. Of course, if the scores did not matter, that would be
one thing, but sometimes the consequences are dire, leading to teachers
being fired.

There are also reasons to worry about scoring criminal defendants rather
than relying on a judge’s discretion. Consider the data pouring into the
algorithms. In part, it comes from police interactions with the
populace, which is known to be uneven, often race-based. The other kind
of input, usually a questionnaire, is also troublesome. Some of them
even ask defendants if their families have a history of being in trouble
with the law, which would be unconstitutional if asked in open court but
gets embedded in the defendant’s score and labelled “objective”.

\[The popularity of these predictive algorithms\] relies on the notion
they are objective, but the algorithms that power the data economy are
based on choices made by fallible human beings. And, while some of them
were made with good intentions, the algorithms encode human prejudice,
misunderstanding, and bias into automatic systems that increasingly
manage our lives.

Predictive Algorithm Scenario #2

**Algorithms as screeners without feedback**

**F**inding work used to be largely a question of whom you knew.
Companies like Kronos brought science into corporate human resources in
part to make the process fairer. The hiring business is becoming
automated, and many of the new programs include personality tests. Such
tests now are used on 60 to 70% of prospective workers in the US.

Defenders of the tests note that they feature lots of questions and that
no single answer can disqualify an applicant. Certain patterns of
answers, however, can and do disqualify them. And we do not know what
those patterns are. We’re not told what the tests are looking for. The
process is entirely opaque. What’s worse, after the model is calibrated
by technical experts, it receives precious little feedback.

Sports provide a good contrast here. Most professional basketball teams
employ data geeks, who run models that analyse players by a series of
metrics, including foot speed, vertical leap, free-throw percentage, and
a host of other variables. Teams rely on these models when deciding
whether or not to recruit players. But if, say, the Los Angeles Lakers
decide to pass on a player because his stats suggest that he won’t
succeed, and then that player subsequently becomes a star, the Lakers
can return to their model to see what they got wrong. Whatever the case,
they can work to improve their model.

Predictive Algorithm Scenario #3

**Seeking fairness**

Naturally, many hiring models attempt to calculate the likelihood that a
job candidate will stick around. Evolv, Inc, helped Xerox scout out
prospects for its call centres, which employ more than 40,000 people.
The churn model took into account some of the metrics you might expect,
including the average time people stuck around on previous jobs. But
they also found some intriguing correlations. People the system
classified as “creative types” tended to stay longer at the job, while
those who scored high on “inquisitiveness” were more likely to set their
questioning minds towards other opportunities.

But the most problematic correlation had to do with geography. Job
applicants who lived farther from the job were more likely to churn.
This makes sense: long commutes are a pain. But Xerox managers noticed
another correlation. Many of the people suffering those long commutes
were coming from poor neighbourhoods. So Xerox, to its credit, removed
that highly correlated churn data from its model. The company sacrificed
a bit of efficiency for fairness.

**Alternate Algorithmic Bias Online Assignment**

1.  List at least 5 things that might have been taken into consideration
    in granting you admission to your college or university.

2.  It is likely that there was a point-based rubric used to predict
    your probable success at this institution based on some of the
    criteria you mentioned in question 1. For example, you might get a
    certain number of points for a high GPA, a certain number for each
    extra-curricular activity etc. Why do you think this method was used
    (if indeed it was)? What are the benefits of this method?

3.  What are the drawbacks of a point-based method (such as described in
    question 2)?

4.  Read at least one of the two following scenarios. After reading, you
    will be asked to comment on a potential additional drawback of a
    purely point-based method.

> **Scenario #1: Algorithms as screeners without feedback**
>
> Finding work used to be largely a question of whom you knew. Companies
> like Kronos brought science into corporate human resources in part to
> make the process fairer. The hiring business is becoming automated,
> and many of the new programs include personality tests. Such tests now
> are used on 60 to 70% of prospective workers in the US.
>
> Defenders of the tests note that they feature lots of questions and
> that no single answer can disqualify an applicant. Certain patterns of
> answers, however, can and do disqualify them. And we do not know what
> those patterns are. We’re not told what the tests are looking for. The
> process is entirely opaque. What’s worse, after the model is
> calibrated by technical experts, it receives precious little feedback.
>
> Sports provide a good contrast here. Most professional basketball
> teams employ data geeks, who run models that analyse players by a
> series of metrics, including foot speed, vertical leap, free-throw
> percentage, and a host of other variables. Teams rely on these models
> when deciding whether or not to recruit players. But if, say, the Los
> Angeles Lakers decide to pass on a player because his stats suggest
> that he won’t succeed, and then that player subsequently becomes a
> star, the Lakers can return to their model to see what they got wrong.
> Whatever the case, they can work to improve their model.
>
>  
>
> **Scenario #3: Seeking fairness**
>
> Naturally, many hiring models attempt to calculate the likelihood that
> a job candidate will stick around. Evolv, Inc, helped Xerox scout out
> prospects for its call centres, which employ more than 40,000 people.
> The churn model took into account some of the metrics you might
> expect, including the average time people stuck around on previous
> jobs. But they also found some intriguing correlations. People the
> system classified as “creative types” tended to stay longer at the
> job, while those who scored high on “inquisitiveness” were more likely
> to set their questioning minds towards other opportunities.
>
> But the most problematic correlation had to do with geography. Job
> applicants who lived farther from the job were more likely to churn.
> This makes sense: long commutes are a pain. But Xerox managers noticed
> another correlation. Many of the people suffering those long commutes
> were coming from poor neighbourhoods. So Xerox, to its credit, removed
> that highly correlated churn data from its model. The company
> sacrificed a bit of efficiency for fairness.
>
> Comment here on the additional drawback discovered:

1.  Point-based algorithms for making decisions are a rudimentary form
    of Artificial Intelligence. Kaplan and Haenlein recently defined AI
    as “a system's ability to correctly interpret external data, to
    learn from such data, and to use those learnings to achieve specific
    goals and tasks through flexible adaptation” Our example of a
    point-based algorithm for college admission is used to predict who
    will most likely succeed at a particular college or university.
    Similarly, each of the other scenarios that you read involved simple
    predictive algorithms.

> We hear frequently of the challenges with AI and even simple
> predictive algorithms share the same issues. Recently, the EU came up
> with a set of guidelines to help diminish the chance of ethical issues
> in such algorithms. As you go through these guidelines, think back on
> our college admissions example and the scenario that you read and see
> if any of them would be helpful to consider if you were in charge of
> developing such an algorithm.
>
> **1 Human agency and oversight**
>
> Users should be able to make informed autonomous decisions regarding
> AI systems. They should be given the knowledge and tools to comprehend
> and interact with AI systems to a satisfactory degree and, where
> possible, be enabled to reasonably self-assess or challenge the
> system. Humans are involved in the design and governance of the system
> including when and where the system is used.
>
> **2 Technical robustness and safety**
>
> Including resilience to attack and security, fall back plan and
> general safety, accuracy, reliability with a range of inputs and
> reproducibility of results with same inputs. The system is well-tested
> during creation and reviewed during use.
>
> **3 Privacy and data governance**
>
> Including respect for privacy, quality and integrity of data, and
> access to data. Data must be ethically cleaned before it is used.
>
> **4 Transparency**
>
> The data sets on which the decision making is based must be
> well-documented with regard to origin and alteration (such as in
> cleaning). Decisions made by AI must be understandable and able to be
> traced. Users should have access to the decision-making process and be
> aware that the decision was made, at least in part, but a computer.
>
> **5 Diversity, non-discrimination and fairness**
>
> Including the avoidance of unfair bias, accessibility and universal
> design, and stakeholder participation. Diversity should be encouraged
> and designers should consider carefully whether past positive results
> (success of people with certain gender, ethnicity, personality etc.)
> were related or unrelated to the success. Avoid propagation of past
> bias. Systems should be user-centric and accessible to the widest
> range of users. People who will be affected by the system should be
> involved throughout the design of the system.
>
> **6 Societal and environmental wellbeing**
>
> Including sustainability and environmental friendliness, social
> impact, society and democracy. Supply chain for the system should be
> sustainable. Its use should impact social relationships in a negative
> way. It should enhance, rather than detract from, a democratic
> society.
>
> **7 Accountability**
>
> This necessitates that mechanisms be put in place to ensure
> responsibility and accountability for AI systems and their outcomes,
> both before and after their development, deployment and use. Potential
> impacts should be minimized and when encountered, they should be
> addressed and there should be a policy for making amends to the
> injured party. The system should be adjusted to eliminate the
> possibility of this happening in the future. When a conflict of
> interest arises, the tradeoffs made should be acknowledged.

1.  Below explain how following at least 2 of the guidelines would have
    benefitted the developers of the previous algorithms that you
    considered to help them avoid bias or other pitfalls. Name the
    guideline and explain how it would have helped.

2.  Do you think that a set of guidelines is a good way to approach
    preventing bias or lack of transparency in algorithms? Why or why
    not?

3.  Comment on one thing that you learned to look out for when creating
    a predictive algorithm

> Peer Review for Algorithmic Bias Assignment

1.  List one important point that the author of the assignment that you
    are reviewing made. Explain why it made an impact on you.

2.  List one comment that the author of the assignment that you are
    reviewing made that you would like to discuss further. Why did that
    comment stick out to you?
